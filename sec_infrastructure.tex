
\section{Infrastructure for Verification of Reproducibility}
\label{sec:infrastructure}

The Data Editor manages the infrastructure needed to access data and code, conducts reproducibility checks, and archives and preserves replication packages. In general, the first two infrastructure pieces are provided by the replication team at Cornell University, the latter primarily by the  \aeadcr{} provided by openICPSR at the University of Michigan, with additional support from the AEA's in-house IT staff. In 2023, the Data Editor continued to explore the use of several other infrastructures for  conducting reproducibility checks and for the preservation of data for replication packages.


\subsection{Pre-publication verification of computational reproducibility}
\label{sec:verification}

\paragraph{The process}

Pre-publicaton verification is conducted by the Data Editor's team at Cornell University. 
Requests for assessment of reproducibility are received and assigned to a team member, who then assesses data availability and compliance with requirements. When some data are available, a full or limited reproducibility check is conducted. If we cannot obtain access to the data or computational resources in a timely fashion, we may reach out to third-parties who can, and request a reproducibility check from them. Once all computations have been completed, a process that can take anywhere from a few minutes to several weeks, a report is compiled, reviewed and approved by the Data Editor, and submitted back to journal editors, who handle most communications with the authors. 

The report will have  one of four possible recommendations (see Table~\ref{tab:responses}), the role of which differs from the stage (``Conditional acceptance'' (CA) or ``Revise and Resubmit'' (R\&R)). During the  \textbf{CA stage}, ``acceptance'' means that no further changes are necessary, and both the manuscript (after copy-editing) and the replication package can be scheduled for publication.%
%
\footnote{Manuscript and replication package are generally published at the same time, though at the request of either editors or authors, the replication package can be published at any time after acceptance.} 
%
However, to streamline processing, we may also recommend an ``acceptance with modifications requested.'' In such cases, the remaining modifications are minor, and can be handled during copy-editing (for instance, a small number of tables need minimal changes) and prior to publication of the replication package (for instance, a fixable error in a program, or a clarification in the README, not affecting any important tables or figures). While we check that authors comply with the request for modifications, no further computational assessment is made. A recommendation of ``Conditional acceptance'' implies that the manuscript needs to go through another round of author revisions (stays in the CA stage), and a revised manuscript and replication package will need to be resubmitted  to address any identified shortcomings. Finally, a recommendation of  ``revise and resubmit'' is recorded when the Data Editor has serious concerns that might warrant that referees and the journal editor have another look at the manuscript. This has never been used, but on occasion, the Data Editor will consult with the journal editor about the right process. 

For some journals, we may also receive a request prior to a conditional acceptance by the journal editor, i.e., during the ``R\&R'' stage. This is regularly used by AEJ:Applied Economics, and may be used for comment articles, at the discretion of the journal editor. During this stage, only two recommendations are habitually used: ``Accept'', indicating that from the point of view of the Data Editor, the replication package and the manuscript can be given a conditional acceptance, and ``Revise and resubmit'', which indicates that there is still substantial work by the authors in order to bring their replication package in compliance. In all cases, the replication package will be reviewed again once the journal editor has given it a conditional acceptance. 



%\begin{table}[t]
\begin{center}
	\captionof{table}{Recommendations}{}
	\label{tab:responses}
	\centering
	\input{tables/jira_response_options_mod}
 \end{center}
%\end{table}


\paragraph{Assessments made}

Between \firstday{} and \lastday{}, the AEA Data Editor team  received
\jiraissues{} requests,  for \jiramcs{} manuscripts.%
%
\footnote{This includes only requests submitted between those dates, and does not take into account in-progress requests on \firstday{}.}
%
Requests typically are channeled to the team by the AEA's journal submission and review system, but others were initiated by authors or editors directly, often while preparing the replication materials. Of these,  \jiraissuescplt{} reports (\jiramcscplt{} manuscripts) were submitted back to editors,\footnote{The balance are either in progress or are not coded in the adminstrative system as having been submitted to ScholarOne, such as replication packages for Papers and Proceedings.} and \jiramcspending{} were completed up to the point of publication of the data deposit, including any post-acceptance modifications.  Table~\ref{tab:responses} shows the distribution of the last recommendation on record for manuscripts as of \lastday{}.  Table~\ref{tab:jirastats} breaks these numbers down by journal, showing the number of requests received (``rcvd'') and  reports completed (``cplt'') in the left panel. The right panel shows the number of manuscripts for which one or more requests were received (``rcvd'') and reports completed (``cplt''). The columns marked ``ext.'' identify cases where we reached out to external replicators, which we discuss later. Finally, the last column identifies manuscripts for which the entire process has been completed, and which are ``pending'' publication.
%

\begin{table}[]
    \caption{Processing Statistics}
    \label{tab:jirastats}
    \begin{threeparttable}
    %\tiny
    \centering
    \input{tables/n_journal_numbers_mod}
    \begin{tablenotes}
    \footnotesize
    \item[] \textit{Notes:} Data for requests received by the AEA Data Editor between \firstday{} and \lastday{}. %AEA P\&P are excluded from this table. 
    See text for details.
    \end{tablenotes}
 \end{threeparttable}
 
   \end{table}





\paragraph{Typical Issues}


\subparagraph{Incomplete data provenance and data availability:} Articles  provide imprecise or incorrect information regarding access to data that is not provided, or for data that is provided. In some cases, authors fail to provide data that should be provided, and in other cases, authors inadvertently provide data for which they do not have redistribution rights. Examples of such data are the World Values Survey, the Panel Study of Income Dynamics, the Socio-Economic Panel, the UN COMTRADE database, and even in one case a Compustat extract. In such cases, authors are required to remove the data, or provide evidence that they have received permission from the data owner to redistribute the data. 

\subparagraph{Specification of computational environment:}
Sufficiently precise descriptions of the required auxiliary packages or libraries, as ``manifest''-like files in R, Python, and Julia, or as "\texttt{setup.do}"-like programs in Stata remain rare. Very few replication packages use containers (``Docker''). We continue to work with some users to leverage such environments when appropriate (see our discussion later under \textit{Computational Infrastructure}).

\subparagraph{Incomplete instructions and manual manipulation:} We observe many packages that do not have a small number of control programs (``master.do'' or similar). Between 28 and 50\% of packages contains such files, the remainder using manual instructions to run multiple code files. Similarly, many authors still manually  save figures and copy  tables. These relatively simple coding practices detract from speedy and efficient reproduction by third parties, including the Data Editor and the authors themselves. We continue to accept packages that do not do this, but are developing guidance and references which should help authors make their packages more streamlined with little extra effort.


\paragraph{Delays} 

A recurring concern expressed by authors, editors, and staff members continue to be delays in processing by the Data Editor, due to the verification process. The median manuscript is reviewed once (Table~\ref{tab:pre:rounds} shows the breakout by journal), but the duration of an evaluation round remains too long, with median times by journal hovering around  90 days. We are working, in coordination with the editors, on making that significantly shorter.
%
%Delays are in part due to training and hiring challenges. We train three times a year for acceptance in the LDI Lab, and continue to have an excellent retention rate of lab members once trained. However, at each of this year's training events, students have been unable to attend at the last minute. Failure to attend the intensive one-day training is highly correlated with later retention and efficiency. We are addressing these issues through increased recruiting for the training, and expansion of the pool of participants.



\begin{table}
    \centering
    \caption{Assessment rounds for completed manuscripts}
    \label{tab:pre:rounds}
    \begin{threeparttable}
    \centering
    \input{tables/n_rounds_mod}
    \begin{tablenotes}
    \footnotesize
    \item[] \textit{Notes:} Data for manuscripts first sent to the AEA Data Editor between \firstday{} and \lastday{}, and for which all rounds have been completed. AEA P\&P, JEP, and JEL are excluded from this table. See text for details. Numbers differ slightly between this table and Table~\ref{tab:jirastats} because they are extracted from two different administrative systems, with different timing cutoffs.
    \end{tablenotes}
 \end{threeparttable}
\end{table}

% \begin{figure}
%     \includegraphics[width=\textwidth]{images/plot_rounds_compare.png}
%     \centering
%     \caption{Comparing rounds per journal between 2021 and 2022\label{fig:rounds}}
% \end{figure}

\paragraph{Disseminating process information}

We continue to improve our documentation, based on careful monitoring of the process, and where more information could be beneficial. Our documentation aims to (a) provide authors with the information as early as possible, when it is still easy to include reproducible practices in projects at relatively low cost and (b) provide authors with the best information, to reduce frictions and uncertainty.  Authors are provided with an informational form upon submission, and a short form, provided upon conditional acceptance, collects salient information about the replication package, but also links to important guidance.%
\footnote{These forms can also be found at \href{https://www.aeaweb.org/journals/data}{aeaweb.org/journals/data}.} 
Authors are required to provide the information defined in the Social Science Data Editors' template ``README'' \citep{READMEv1.1.0}, though they are free to use their own documents, as long as all the information is available.

\paragraph{Computational Infrastructure}

Most replication packages are computationally verified by replicators on the computers available to the Data Editor at the Cornell University Economics Department and the ILR School. The majority are handled on the Windows Server systems of the Cornell Center for Social Sciences, while some are run on the Linux-based Bioinformatics cluster. Occasionally, personal macOS laptops are used. Systems can handle memory requirements up to 1024 GB or up to 100 cores. 

While these systems are fairly standard, they cannot cover all scenarios described in authors' computational requirements. Furthermore, these systems, much like the authors' own systems, are not shareable more broadly, and thus sometimes make it difficult to control for specific requirements, or to share error messages in the most reproducible way.  

We continue to leverage additional computational environments. We have used \curlcite{https://codeocean.com}{CodeOcean} \citep{clyburne-sherin_computational_2019} both to collaboratively work with some authors on solving the problems in   partially successful reproduction efforts, and to publish reproducible ``capsules.'' We also work with the team behind ``WholeTale'' \citep{BrinckmanFutureGener.Comput.Syst.2018}. Both {{CodeOcean}} and WholeTale rely on containerization, often known under the commercial name ``Docker,'' which can be independently used to precisely define and then share computational environments. We use containers through CodeOcean and or WholeTale, when appropriate, and on the Cornell-based resources, in particular when storage or compute resources are insufficient at the public providers. Sample code  can be found at the \purlcite{https://github.com/AEADataEditor/}{AEA Data Editor's Github repository} Pre-configured Stata and manuscript-specific Docker images can be found at Docker Hub.\footnote{Generic images are at \href{https://hub.docker.com/u/dataeditors}{hub.docker.com/u/dataeditors}, specific images at \href{https://hub.docker.com/u/aeadataeditor}{hub.docker.com/u/aeadataeditor}.} A more expansive overview of containerization issues in economics can be found at \citet{aea_data_editor_use_2021}. Common guidance for economists is forthcoming in collaboration with other data editors.


\subsection{Archive for Replication Packages}


\begin{table}[t]
    \centering
    \caption{Deposit statistics}
    \label{tab:webstats}
     \begin{threeparttable}
     \input{tables/n_webstats}
 
    \begin{tablenotes}
    \footnotesize
    \item[] \textit{Note}: Unit of observation are deposits at the named repository. Columns 1-3 are for all currently published deposits as of \pkglastday{}. Columns 4-6 are for deposits made between \firstday{} and \pkglastday{}. The number of uploads may not correspond to the number of manuscripts processed by the Data Editor team. Not all uploads have been published yet. 
    \end{tablenotes}
    \end{threeparttable}
\end{table}


The default archive for replication packages accompanying articles in AEA journals is the \aeadcr{}. Deposit instructions are provided on the Data Editor's website, and provided to authors upon conditional acceptance. However, it is not the only acceptable archive, as we discuss below.


Table~\ref{tab:webstats} shows statistics for all currently published replication packages at the \aeadcr{}.  There are currently  
\icpsrtotalPublished{} published replication packages.  Between \firstday{} and \pkglastday{}, \pkgcount{} deposits were made, with over \pkgfilesT{} thousand files and over 1 TB of data. 

In the past year, the median package size was  \pkgsizemedian{} MB, but a significant number of packages (\pkgsizetwog{} percent) had  packages larger than 2GB. \pkgsizetwentyg{} percent of deposits were larger than 20GB. 
%
Some packages have more than 1,000 files, hitting a technical constraint on openICPSR. Provision of opaque ZIP files are generally prohibited. Instructions on how to proceed when file numbers are large, while maintaining maximum visibility onto the file and package structure, are provided on our website. Authors with large packages, or packages with more than 1,000 files, should contact the AEA Data Editor. Depositing at other trusted repositories is one option, described in the next section.

Since the migration to the \aeadcr{}, these replication packages have been viewed \icpsrtotalViewsM{} million times and downloaded nearly \icpsrtotalDownloadsHT{} hundred thousand times.




%\subsection{Migrating Historical Supplements}
%\label{sec:migration}
%
%We  migrated the bulk of historical data (and code) supplements in 2019, from ZIP files stored on AEA servers to the \aeadcr{}. A few dozen replication packages are still awaiting migration. Most of these packages either have a very large number of files that surpass technical limitations of the \aeadcr{}, or turned out to be corrupted or illegible in their original version. We are still working on migrating these as resources permit. 
%
%% report on UMich metadata project
%% data provided by email
%
%We reported last year on a project with a research team at the University of Michigan, which solicited metadata improvements to migrated replication packages.  911 researchers provided additional metadata for 522 studies, which were incorporated into the \aeadcr{} in October 2021. We are currently assessing the impact of the improved metadata on findability of such packages.



\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{images/plot_filesize_dist.png} 
    \caption{Size distribution of replication packages deposited at openICPSR between  \firstday{} to \lastday{}, top-coded at 10GB.}
    \label{fig:size_packages}
\end{figure}


\subsection{Third-party repositories}

The \ac{DCAP} allows for code and data  to be deposited at other trusted repositories, as long as all other elements of the \ac{DCAP} are complied with. In fact, authors are \textit{discouraged} from duplicating deposits they have made elsewhere. This is intended to allow authors to create replication packages prior to submitting at the AEA's journals, or any other journal, as a component of a reproducible workflow and possibly in compliance with funder data management policies. Examples of other general purpose repositories include the \urlcite{https://dataverse.harvard.edu/}{Harvard Dataverse} and \purlcite{https://zenodo.org/}{Zenodo}. Authors depositing on Zenodo can request inclusion in the  ``AEA community'' at \href{https://zenodo.org/communities/aeajournals/}{zenodo.org/communities/aeajournals/}. For example, we work with authors to deposit  partial data packages on Zenodo when the size of the data files surpasses 30 GB. Some of these are deposited on Zenodo, where the  ``AEA community'' is available. Table~\ref{tab:webstats} shows a small number of very large packages on Zenodo, with \zenodototalSizeGB{}~GB of data  deposited in only \zenodototalPublished{} packages. 


More specific repositories that only allow specific authors to deposit (but anybody to obtain data from), such as the \purlcite{https://snd.gu.se/en}{Swedish National Data Service} or the new \curlcite{https://reproducibility.worldbank.org/}{World Bank Reproducible Research Repository} have been used more recently for partial or complete replication packages. In many of these cases, the Data Editor has actively assisted authors in preparing  data archives, and shared tools that make such data publication easier (see also next section).%
\footnote{Code to support uploading large quantities of data to Zenodo via the Zenodo API, originally created by LDI Lab Member Vansh Gupta, can be found at \href{https://github.com/AEADataEditor/Upload-to-Zenodo}{github.com/AEADataEditor/Upload-to-Zenodo}.}  Third-party repositories are linked to the main \aeadcr{} deposit, and are cited in the main article when appropriate. Authors wishing to deposit replication packages early in the research lifecycle are encouraged to consult the \curlcite{https://social-science-data-editors.github.io/}{Social Science Data Editors website} where links to trusted repositories are provided.  
